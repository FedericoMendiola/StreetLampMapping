# -*- coding: utf-8 -*-
"""Prog_Depth.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RGH7amkQ6kjt49KhmVtjwTcRJoC_4qlC
"""

import os

os.makedirs('/content/data/images', exist_ok=True)
os.makedirs('/content/results/depth_maps', exist_ok=True)
print("Struttura delle directory completata!")

from google.colab import drive
drive.mount('/content/drive')

# Copia il dataset da Drive alla directory di lavoro
!cp -r /content/drive/MyDrive/_StreetLampMappingPJ/datasets/* /content/data/images/

!git clone https://github.com/nianticlabs/monodepth2.git
os.chdir('monodepth2')

from __future__ import absolute_import, division, print_function

import numpy as np
import PIL.Image as pil
import matplotlib.pyplot as plt
import pandas as pd
from PIL import ImageDraw

import torch
from torchvision import transforms

import networks
from utils import download_model_if_doesnt_exist

model_name = 'mono_1024x320'

download_model_if_doesnt_exist(model_name)
encoder_path = os.path.join("models", model_name, "encoder.pth")
depth_decoder_path = os.path.join("models", model_name, "depth.pth")

# LOADING PRETRAINED MODEL
encoder = networks.ResnetEncoder(18, False)
depth_decoder = networks.DepthDecoder(num_ch_enc=encoder.num_ch_enc, scales=range(4))

loaded_dict_enc = torch.load(encoder_path, map_location='cpu')
filtered_dict_enc = {k: v for k, v in loaded_dict_enc.items() if k in encoder.state_dict()}
encoder.load_state_dict(filtered_dict_enc)

loaded_dict = torch.load(depth_decoder_path, map_location='cpu')
depth_decoder.load_state_dict(loaded_dict)

def predict(image_path):
    input_image = pil.open(image_path).convert('RGB')
    original_width, original_height = input_image.size

    feed_height = loaded_dict_enc['height']
    feed_width = loaded_dict_enc['width']
    input_image_resized = input_image.resize((feed_width, feed_height), pil.LANCZOS)

    input_image_pytorch = transforms.ToTensor()(input_image_resized).unsqueeze(0)

    with torch.no_grad():
        features = encoder(input_image_pytorch)
        outputs = depth_decoder(features)
    disp = outputs[("disp", 0)]

    disp_resized = torch.nn.functional.interpolate(disp,
        (original_height, original_width), mode="bilinear", align_corners=False)

    # Saving colormapped depth image
    disp_resized_np = disp_resized.squeeze().cpu().numpy()
    disp_resized_normalized = (255 * (disp_resized_np - disp_resized_np.min()) / (disp_resized_np.max() - disp_resized_np.min())).astype(np.uint8)
    vmax = np.percentile(disp_resized_np, 95)

    #plt.figure(figsize=(10, 10))
    #plt.subplot(211)
    #plt.imshow(input_image)
    #plt.axis('off')

    #plt.subplot(212)
    #plt.imshow(disp_resized_np, cmap='magma', vmax=vmax)
    #plt.axis('off')

    return disp_resized_normalized

det_dir = '/content/data/images'
res_dir = '/content/results/depth_maps'
os.makedirs(res_dir, exist_ok=True)

lst = os.listdir(det_dir)
for i in range(len(lst)):
    img_file = 'img' + str(i+1) + '.png'
    img_path = os.path.join(det_dir, img_file)
    res_path = os.path.join(res_dir, img_file)

    out = predict(img_path)
    depth_image = pil.fromarray(out).save(res_path)
    print(f"[{i+1}/{len(lst)}]Processed {img_file}, results saved in {res_path}")

print(f"Esecuzione completata con successo! Mappa di profondit√† salvata in: {res_path}")

input_image = pil.open(img_path).convert('RGB')
draw = ImageDraw.Draw(input_image)
fig, axe = plt.subplots(1, figsize=(10,10))
vmax = np.percentile(out, 95)
axe.imshow(out, cmap='magma', vmax=vmax)
axe.imshow(input_image, alpha=0.4)
plt.show()
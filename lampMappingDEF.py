# -*- coding: utf-8 -*-
"""Mendiola_LampMapping.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/189Ux569m-VovEFu8xuv0p5yTplYGGyYj
"""

!pip install ultralytics roboflow matplotlib numpy opencv-python tqdm

import os
from roboflow import Roboflow
import json

!git clone https://github.com/nianticlabs/monodepth2.git
os.chdir('monodepth2')

from __future__ import absolute_import, division, print_function
import numpy as np
import PIL.Image as pil
import matplotlib.pyplot as plt
import pandas as pd
from PIL import ImageDraw
import torch
from torchvision import transforms
import networks
from utils import download_model_if_doesnt_exist
from mpl_toolkits.mplot3d import Axes3D

# Percorsi
data_dir = '/content/data/images'  # Immagini di input
res_det = '/content/results/detected'  # Output rilevamento
res_dep = '/content/results/depth_maps'  # Output profondità
res_map = '/content/results/mapped_points'  # Output mappatura 3d
# Creazione delle directory principali
os.makedirs(data_dir, exist_ok=True)  # Dataset originale
os.makedirs(res_det, exist_ok=True)  # Bounding box rilevate da YOLO
os.makedirs(res_dep, exist_ok=True)  # Mappe di profondità
os.makedirs(res_map, exist_ok=True)  # Coordinate 3D
print("Struttura delle directory completata!")

from google.colab import drive
drive.mount('/content/drive')

# Copia il dataset da Drive alla directory di lavoro
!cp -r /content/drive/MyDrive/_StreetLampMappingPJ/datasets/* /content/data/images/

# Configurazione di Roboflow
rf = Roboflow(api_key="4kUWUoS1Lmb0FiHfjPMB")
project = rf.workspace().project("street-lamp-v2")
model = project.version("1").model

# File per salvare i dati delle bounding box
bbox_data_path = os.path.join(res_det, "detection_results.json")

# Dati delle bounding box
detection_results = []
i = 0

# Applica il rilevamento
lst = os.listdir(data_dir)
for i in range(len(lst)):
    img_file = 'img' + str(i+1) + '.png'
    img_path = os.path.join(data_dir, img_file)
    res_path = os.path.join(res_det, img_file)

    # Rilevamento dei lampioni
    predictions = model.predict(img_path)

    # Salva immagine annotata
    annotated_img = predictions.save(res_path)

    # Estrai le bounding box e salva i dati
    for pred in predictions:
        detection_results.append({
            "image": img_file,
            "class": pred['class'],
            "confidence": pred['confidence'],
            "x_min": pred['x'],
            "y_min": pred['y'],
            "width": pred['width'],
            "height": pred['height']
        })
    print(f"[{i+1}/{len(lst)}]Processed {img_file}, results saved in {res_det}")
    i += 1

# Salva i dati delle bounding box in formato JSON
with open(bbox_data_path, "w") as json_file:
    json.dump(detection_results, json_file, indent=4)

print(f"Tutti i risultati delle bounding box sono stati salvati in: {bbox_data_path}")

# Copia il dataset da Drive alla directory di lavoro
!cp -r /content/results/detected /content/drive/MyDrive/_StreetLampMappingPJ/detectedImage/

model_name = 'mono_1024x320'

download_model_if_doesnt_exist(model_name)
encoder_path = os.path.join("models", model_name, "encoder.pth")
depth_decoder_path = os.path.join("models", model_name, "depth.pth")

# LOADING PRETRAINED MODEL
encoder = networks.ResnetEncoder(18, False)
depth_decoder = networks.DepthDecoder(num_ch_enc=encoder.num_ch_enc, scales=range(4))

loaded_dict_enc = torch.load(encoder_path, map_location='cpu')
filtered_dict_enc = {k: v for k, v in loaded_dict_enc.items() if k in encoder.state_dict()}
encoder.load_state_dict(filtered_dict_enc)

loaded_dict = torch.load(depth_decoder_path, map_location='cpu')
depth_decoder.load_state_dict(loaded_dict)

def predict(image_path):
    input_image = pil.open(image_path).convert('RGB')
    original_width, original_height = input_image.size

    feed_height = loaded_dict_enc['height']
    feed_width = loaded_dict_enc['width']
    input_image_resized = input_image.resize((feed_width, feed_height), pil.LANCZOS)

    input_image_pytorch = transforms.ToTensor()(input_image_resized).unsqueeze(0)

    with torch.no_grad():
        features = encoder(input_image_pytorch)
        outputs = depth_decoder(features)
    disp = outputs[("disp", 0)]

    disp_resized = torch.nn.functional.interpolate(disp,
        (original_height, original_width), mode="bilinear", align_corners=False)

    # Saving colormapped depth image
    disp_resized_np = disp_resized.squeeze().cpu().numpy()
    disp_resized_normalized = (255 * (disp_resized_np - disp_resized_np.min()) / (disp_resized_np.max() - disp_resized_np.min())).astype(np.uint8)
    vmax = np.percentile(disp_resized_np, 95)

    #plt.figure(figsize=(10, 10))
    #plt.subplot(211)
    #plt.imshow(input_image)
    #plt.axis('off')

    #plt.subplot(212)
    #plt.imshow(disp_resized_np, cmap='magma', vmax=vmax)
    #plt.axis('off')

    return disp_resized_normalized

lst = os.listdir(data_dir)
for i in range(len(lst)):
    img_file = 'img' + str(i+1) + '.png'
    img_path = os.path.join(data_dir, img_file)
    res_path = os.path.join(res_dep, img_file)

    out = predict(img_path)
    depth_image = pil.fromarray(out).save(res_path)
    print(f"[{i+1}/{len(lst)}]Processed {img_file}, results saved in {res_dep}")
    i += 1

print(f"Esecuzione completata con successo! Mappa di profondità salvata in: {res_dep}")

# Copia il dataset da Drive alla directory di lavoro
!cp -r /content/results/depth_maps /content/drive/MyDrive/_StreetLampMappingPJ/depthImage/

img_path = os.path.join(data_dir, 'img342.png')
input_image = pil.open(img_path).convert('RGB')
draw = ImageDraw.Draw(input_image)
fig, axe = plt.subplots(1, figsize=(10,10))
vmax = np.percentile(out, 95)
axe.imshow(out, cmap='magma', vmax=vmax)
axe.imshow(input_image, alpha=0.4)
plt.show()

# Funzione per calcolare le coordinate 3D
def calculate_3d_coordinates(bbox, depth_map):
    x, y, w, h = bbox["x_min"], bbox["y_min"], bbox["width"], bbox["height"]
    if int(y + h) > depth_map.shape[0] or int(x + w) > depth_map.shape[1]:
        return None
    depth = depth_map[int(y):int(y + h), int(x):int(x + w)].mean()
    return [x + w / 2, y + h / 2, depth]

# Carica i dati delle bounding box
with open('/content/results/detected/detection_results.json', "r") as f:
    detections = json.load(f)

lst = os.listdir(res_dep)
for i in range(len(lst)):
    img_file = f'img{i + 1}.png'
    depth_map = np.array(pil.open(os.path.join(res_dep, img_file)))
    points_3d = [
        calculate_3d_coordinates(bbox, depth_map)
        for bbox in detections if bbox["image"] == img_file
    ]
    points_3d = [p for p in points_3d if p is not None]
    if points_3d:
      np.save(os.path.join(res_map, f"{img_file.split('.')[0]}.npy"), points_3d)
    print(f"[{i+1}/{len(lst)}]Saved 3D points for {img_file}, results saved in {res_map}")

# Copia il dataset da Drive alla directory di lavoro
!cp -r /content/results/mapped_points /content/drive/MyDrive/_StreetLampMappingPJ/mappedImage/

# Offset per simulare la progressione temporale
time_offset = 1

for i, file in enumerate(sorted(os.listdir(res_map))):
    points = np.load(os.path.join(res_map, file))
    if points.ndim == 2 and points.shape[1] == 3:
        points[:, 0] += i * time_offset
        np.save(os.path.join(res_map, file), points)

# Creazione del grafico 3D
fig = plt.figure(figsize=(10, 10))
ax = fig.add_subplot(111, projection='3d')

# Carica e plotta i punti
for file in sorted(os.listdir(res_map)):
    points = np.load(os.path.join(res_map, file))
    if points.ndim != 2 or points.shape[1] != 3:
        continue
    ax.scatter(points[:, 0], points[:, 1], points[:, 2], label=file)

ax.set_xlabel("X (pixel)")
ax.set_ylabel("Y (pixel)")
ax.set_zlabel("Z (profondità)")
ax.set_title("Visualizzazione 3D dei Lampioni")
#plt.legend()
plt.show()

# Esempio di caricamento di un file di punti 3D
import numpy as np
points_3d = np.load('/content/results/mapped_points/img102.npy')

# Statistiche
depths = points_3d[:, 2]
print(f"Numero di lampioni: {len(depths)}")
print(f"Profondità media: {np.mean(depths):.2f}")
print(f"Deviazione standard: {np.std(depths):.2f}")

# Visualizza immagini annotate con sovrapposizione delle mappe di profondità
fig, axes = plt.subplots(4, 3, figsize=(15, 10))
axes = axes.ravel()

for i, img_file in enumerate(sorted(os.listdir(res_dep)[:12])):
    depth_path = os.path.join(res_dep, img_file)
    annotated_path = os.path.join(res_det, img_file)

    depth_map = np.array(pil.open(depth_path))
    annotated_img = pil.open(annotated_path)

    vmax = np.percentile(depth_map, 95)  # Limita i valori di profondità
    axes[i].imshow(depth_map, cmap='magma', vmax=vmax, alpha=0.5)
    axes[i].imshow(annotated_img, alpha=0.5)
    axes[i].set_title(f"Sovrapposizione: {img_file}")
    axes[i].axis('off')

plt.tight_layout()
plt.show()